# Seq2Seq Attention Comparison Documentation

<div align="center">

**Comprehensive Guide to Bahdanau vs Luong Attention Mechanisms**

[Getting Started](getting-started/quickstart.md) â€¢
[Theory](theory/attention-mechanisms.md) â€¢
[API Reference](api/overview.md) â€¢
[Tutorials](tutorials/training-custom-data.md)

</div>

---

## ðŸ“‹ Overview

This documentation covers the implementation and comparison of two seminal attention mechanisms for neural machine translation:

| Mechanism | Paper | Year | Key Innovation |
|-----------|-------|------|----------------|
| **Bahdanau** | [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473) | 2014 | Additive attention with learned alignment |
| **Luong** | [Effective Approaches to Attention-based Neural Machine Translation](https://arxiv.org/abs/1508.04025) | 2015 | Multiplicative attention variants |

## ðŸš€ Quick Navigation

### Getting Started
- [Installation Guide](getting-started/installation.md) - Set up your environment
- [Quickstart](getting-started/quickstart.md) - Run your first comparison in 5 minutes
- [Configuration](getting-started/configuration.md) - Customize training parameters

### Theory
- [Attention Mechanisms](theory/attention-mechanisms.md) - Why attention matters
- [Bahdanau Attention](theory/bahdanau-attention.md) - Additive attention deep dive
- [Luong Attention](theory/luong-attention.md) - Multiplicative attention variants
- [Seq2Seq Architecture](theory/seq2seq-architecture.md) - Encoder-decoder fundamentals

### API Reference
- [API Overview](api/overview.md) - Class hierarchy and module structure
- [Attention Layers](api/attention-layers.md) - `BahdanauAttention` & `LuongAttention`
- [Encoder & Decoder](api/encoder-decoder.md) - Core model components
- [Training Functions](api/training-functions.md) - Training utilities
- [Evaluation Functions](api/evaluation-functions.md) - Testing and visualization

### Tutorials
- [Training on Custom Data](tutorials/training-custom-data.md) - Use your own datasets
- [Visualizing Attention](tutorials/visualizing-attention.md) - Create attention heatmaps
- [Extending Models](tutorials/extending-models.md) - Add new attention types

### Results
- [Experimental Setup](results/experimental-setup.md) - Reproducibility details
- [Performance Analysis](results/performance-analysis.md) - Comparative results
- [Attention Visualizations](results/attention-visualizations.md) - Visual gallery

---

## ðŸ”— Repository

**GitHub**: [Abdulrahmann-Omar/NLP-Assingment-1](https://github.com/Abdulrahmann-Omar/NLP-Assingment-1)

---

*Zewail City of Science and Technology - NLP Course, 4th Year*
