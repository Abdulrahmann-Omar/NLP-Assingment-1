{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seq2Seq Attention Mechanisms Comparison\n",
    "## Bahdanau (Additive) vs Luong (Multiplicative) Attention\n",
    "\n",
    "This notebook implements and compares two attention mechanisms:\n",
    "1. **Bahdanau Attention** (Additive Attention)\n",
    "2. **Luong Attention** (Multiplicative Attention)\n",
    "\n",
    "For English-Portuguese machine translation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import unicodedata\n",
    "import re\n",
    "import os\n",
    "import io\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unicode_to_ascii(s):\n",
    "    \"\"\"Convert unicode to ASCII\"\"\"\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "                   if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "def preprocess_sentence(w):\n",
    "    \"\"\"Preprocess a sentence\"\"\"\n",
    "    w = unicode_to_ascii(w.lower().strip())\n",
    "    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
    "    w = re.sub(r'[\" \"]+', \" \", w)\n",
    "    w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
    "    w = w.strip()\n",
    "    w = '\u003cstart\u003e ' + w + ' \u003cend\u003e'\n",
    "    return w\n",
    "\n",
    "def create_dataset(path, num_examples=None):\n",
    "    lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
    "    word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')[:2]] \n",
    "                  for l in lines[:num_examples]]\n",
    "    return zip(*word_pairs)\n",
    "\n",
    "def tokenize(lang):\n",
    "    lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "    lang_tokenizer.fit_on_texts(lang)\n",
    "    tensor = lang_tokenizer.texts_to_sequences(lang)\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
    "    return tensor, lang_tokenizer\n",
    "\n",
    "def load_dataset(path, num_examples=None):\n",
    "    targ_lang, inp_lang = create_dataset(path, num_examples)\n",
    "    input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n",
    "    target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n",
    "    return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Mechanism Implementations\n",
    "\n",
    "### 1. Bahdanau Attention (Additive)\n",
    "Formula: `score(s_t, h_i) = v^T * tanh(W_s * s_t + W_h * h_i)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    \"\"\"Bahdanau Attention (Additive)\"\"\"\n",
    "    \n",
    "    def __init__(self, units, name=\"bahdanau_attention\"):\n",
    "        super(BahdanauAttention, self).__init__(name=name)\n",
    "        self.W1 = tf.keras.layers.Dense(units, name=\"W1\")\n",
    "        self.W2 = tf.keras.layers.Dense(units, name=\"W2\")\n",
    "        self.V = tf.keras.layers.Dense(1, name=\"V\")\n",
    "    \n",
    "    def call(self, query, values):\n",
    "        # query: decoder hidden state (batch, hidden)\n",
    "        # values: encoder outputs (batch, seq_len, hidden)\n",
    "        \n",
    "        query_with_time_axis = tf.expand_dims(query, 1)\n",
    "        score = self.V(tf.nn.tanh(\n",
    "            self.W1(query_with_time_axis) + self.W2(values)\n",
    "        ))\n",
    "        \n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        context_vector = attention_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        \n",
    "        return context_vector, attention_weights\n",
    "\n",
    "print(\"✓ Bahdanau Attention defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Luong Attention (Multiplicative)\n",
    "Formula: `score(s_t, h_i) = s_t^T * W_a * h_i` (general)\n",
    "\n",
    "Three variants:\n",
    "- **dot**: `s_t^T * h_i`\n",
    "- **general**: `s_t^T * W_a * h_i`\n",
    "- **concat**: `v^T * tanh(W_a * [s_t; h_i])`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LuongAttention(tf.keras.layers.Layer):\n",
    "    \"\"\"Luong Attention (Multiplicative)\"\"\"\n",
    "    \n",
    "    def __init__(self, units, score_type='general', name=\"luong_attention\"):\n",
    "        super(LuongAttention, self).__init__(name=name)\n",
    "        self.score_type = score_type\n",
    "        self.units = units\n",
    "        \n",
    "        if score_type == 'general':\n",
    "            self.W = tf.keras.layers.Dense(units, use_bias=False, name=\"W_general\")\n",
    "        elif score_type == 'concat':\n",
    "            self.W = tf.keras.layers.Dense(units, name=\"W_concat\")\n",
    "            self.V = tf.keras.layers.Dense(1, name=\"V_concat\")\n",
    "    \n",
    "    def call(self, query, values):\n",
    "        # query: decoder hidden state (batch, hidden)\n",
    "        # values: encoder outputs (batch, seq_len, hidden)\n",
    "        \n",
    "        query_with_time_axis = tf.expand_dims(query, 1)\n",
    "        \n",
    "        if self.score_type == 'dot':\n",
    "            score = tf.matmul(query_with_time_axis, values, transpose_b=True)\n",
    "            score = tf.squeeze(score, axis=1)\n",
    "            score = tf.expand_dims(score, axis=-1)\n",
    "            \n",
    "        elif self.score_type == 'general':\n",
    "            score = tf.matmul(query_with_time_axis, self.W(values), transpose_b=True)\n",
    "            score = tf.squeeze(score, axis=1)\n",
    "            score = tf.expand_dims(score, axis=-1)\n",
    "            \n",
    "        elif self.score_type == 'concat':\n",
    "            query_tiled = tf.tile(query_with_time_axis, [1, tf.shape(values)[1], 1])\n",
    "            concat = tf.concat([query_tiled, values], axis=-1)\n",
    "            score = self.V(tf.nn.tanh(self.W(concat)))\n",
    "        \n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        context_vector = attention_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        \n",
    "        return context_vector, attention_weights\n",
    "\n",
    "print(\"✓ Luong Attention defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder and Decoder Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(\n",
    "            enc_units,\n",
    "            return_sequences=True,\n",
    "            return_state=True,\n",
    "            recurrent_initializer='glorot_uniform'\n",
    "        )\n",
    "    \n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        output, state = self.gru(x, initial_state=hidden)\n",
    "        return output, state\n",
    "    \n",
    "    def initialize_hidden_state(self, batch_size=None):\n",
    "        if batch_size is None:\n",
    "            batch_size = self.batch_sz\n",
    "        return tf.zeros((batch_size, self.enc_units))\n",
    "\n",
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz, attention_layer):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(\n",
    "            dec_units,\n",
    "            return_sequences=True,\n",
    "            return_state=True,\n",
    "            recurrent_initializer='glorot_uniform'\n",
    "        )\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "        self.attention = attention_layer\n",
    "    \n",
    "    def call(self, x, hidden, enc_output):\n",
    "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "        x = self.embedding(x)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "        output, state = self.gru(x)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "        x = self.fc(output)\n",
    "        return x, state, attention_weights\n",
    "\n",
    "print(\"✓ Encoder and Decoder defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(real, pred, loss_object):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    return tf.reduce_mean(loss_)\n",
    "\n",
    "@tf.function\n",
    "def train_step(inp, targ, enc_hidden, encoder, decoder, targ_lang, optimizer, loss_object):\n",
    "    loss = 0\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "        dec_hidden = enc_hidden\n",
    "        dec_input = tf.expand_dims([targ_lang.word_index['\u003cstart\u003e']] * inp.shape[0], 1)\n",
    "        \n",
    "        for t in range(1, targ.shape[1]):\n",
    "            predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "            loss += loss_function(targ[:, t], predictions, loss_object)\n",
    "            dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "    \n",
    "    batch_loss = (loss / int(targ.shape[1]))\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "    \n",
    "    return batch_loss\n",
    "\n",
    "print(\"✓ Training functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "BATCH_SIZE = 64\n",
    "EMBEDDING_DIM = 256\n",
    "UNITS = 512\n",
    "NUM_EXAMPLES = 30000  # Use subset for faster training\n",
    "EPOCHS = 10\n",
    "\n",
    "# Download dataset\n",
    "path_to_zip = tf.keras.utils.get_file(\n",
    "    'por-eng.zip',\n",
    "    origin='http://storage.googleapis.com/download.tensorflow.org/data/por-eng.zip',\n",
    "    extract=True\n",
    ")\n",
    "path_to_file = os.path.dirname(path_to_zip) + \"/por-eng/por.txt\"\n",
    "\n",
    "print(f\"Dataset path: {path_to_file}\")\n",
    "print(f\"Using {NUM_EXAMPLES} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "input_tensor, target_tensor, inp_lang, targ_lang = load_dataset(path_to_file, NUM_EXAMPLES)\n",
    "\n",
    "max_length_targ, max_length_inp = target_tensor.shape[1], input_tensor.shape[1]\n",
    "print(f\"Max input length: {max_length_inp}\")\n",
    "print(f\"Max target length: {max_length_targ}\")\n",
    "\n",
    "# Train/val split\n",
    "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = \\\n",
    "    train_test_split(input_tensor, target_tensor, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"\\nTraining samples: {len(input_tensor_train)}\")\n",
    "print(f\"Validation samples: {len(input_tensor_val)}\")\n",
    "\n",
    "# Vocabulary sizes\n",
    "vocab_inp_size = len(inp_lang.word_index) + 1\n",
    "vocab_tar_size = len(targ_lang.word_index) + 1\n",
    "print(f\"\\nInput vocabulary: {vocab_inp_size}\")\n",
    "print(f\"Target vocabulary: {vocab_tar_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TensorFlow dataset\n",
    "BUFFER_SIZE = len(input_tensor_train)\n",
    "steps_per_epoch = len(input_tensor_train) // BATCH_SIZE\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (input_tensor_train, target_tensor_train)\n",
    ").shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "print(f\"Steps per epoch: {steps_per_epoch}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1: Bahdanau Attention\n",
    "### Build and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"CREATING BAHDANAU ATTENTION MODEL\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "encoder_bahdanau = Encoder(vocab_inp_size, EMBEDDING_DIM, UNITS, BATCH_SIZE)\n",
    "bahdanau_attention = BahdanauAttention(UNITS)\n",
    "decoder_bahdanau = Decoder(vocab_tar_size, EMBEDDING_DIM, UNITS, BATCH_SIZE, bahdanau_attention)\n",
    "\n",
    "optimizer_bahdanau = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "print(\"✓ Bahdanau model created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Bahdanau model\n",
    "history_bahdanau = {'loss': []}\n",
    "\n",
    "print(\"\\nTraining Bahdanau Attention Model...\")\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "    enc_hidden = encoder_bahdanau.initialize_hidden_state(BATCH_SIZE)\n",
    "    total_loss = 0\n",
    "    \n",
    "    for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "        batch_loss = train_step(inp, targ, enc_hidden, encoder_bahdanau, decoder_bahdanau,\n",
    "                               targ_lang, optimizer_bahdanau, loss_object)\n",
    "        total_loss += batch_loss\n",
    "        \n",
    "        if batch % 100 == 0:\n",
    "            print(f'  Epoch {epoch + 1} Batch {batch} Loss {batch_loss.numpy():.4f}')\n",
    "    \n",
    "    epoch_loss = total_loss / steps_per_epoch\n",
    "    history_bahdanau['loss'].append(epoch_loss.numpy())\n",
    "    \n",
    "    print(f'Epoch {epoch + 1} Loss {epoch_loss:.4f} Time: {time.time() - start:.2f}s\\n')\n",
    "\n",
    "print(\"✓ Bahdanau training complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2: Luong Attention\n",
    "### Build and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"CREATING LUONG ATTENTION MODEL\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "encoder_luong = Encoder(vocab_inp_size, EMBEDDING_DIM, UNITS, BATCH_SIZE)\n",
    "luong_attention = LuongAttention(UNITS, score_type='general')\n",
    "decoder_luong = Decoder(vocab_tar_size, EMBEDDING_DIM, UNITS, BATCH_SIZE, luong_attention)\n",
    "\n",
    "optimizer_luong = tf.keras.optimizers.Adam()\n",
    "\n",
    "print(\"✓ Luong model created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Luong model\n",
    "history_luong = {'loss': []}\n",
    "\n",
    "print(\"\\nTraining Luong Attention Model...\")\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "    enc_hidden = encoder_luong.initialize_hidden_state(BATCH_SIZE)\n",
    "    total_loss = 0\n",
    "    \n",
    "    for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "        batch_loss = train_step(inp, targ, enc_hidden, encoder_luong, decoder_luong,\n",
    "                               targ_lang, optimizer_luong, loss_object)\n",
    "        total_loss += batch_loss\n",
    "        \n",
    "        if batch % 100 == 0:\n",
    "            print(f'  Epoch {epoch + 1} Batch {batch} Loss {batch_loss.numpy():.4f}')\n",
    "    \n",
    "    epoch_loss = total_loss / steps_per_epoch\n",
    "    history_luong['loss'].append(epoch_loss.numpy())\n",
    "    \n",
    "    print(f'Epoch {epoch + 1} Loss {epoch_loss:.4f} Time: {time.time() - start:.2f}s\\n')\n",
    "\n",
    "print(\"✓ Luong training complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training loss comparison\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "# Loss curves\n",
    "plt.subplot(1, 2, 1)\n",
    "epochs_range = range(1, len(history_bahdanau['loss']) + 1)\n",
    "plt.plot(epochs_range, history_bahdanau['loss'], 'b-o', label='Bahdanau Attention', linewidth=2)\n",
    "plt.plot(epochs_range, history_luong['loss'], 'r-s', label='Luong Attention', linewidth=2)\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Loss', fontsize=12)\n",
    "plt.title('Training Loss Comparison', fontsize=14)\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Final loss comparison\n",
    "plt.subplot(1, 2, 2)\n",
    "final_losses = [history_bahdanau['loss'][-1], history_luong['loss'][-1]]\n",
    "bars = plt.bar(['Bahdanau', 'Luong'], final_losses, color=['steelblue', 'coral'], edgecolor='black')\n",
    "plt.ylabel('Final Loss', fontsize=12)\n",
    "plt.title('Final Training Loss', fontsize=14)\n",
    "for bar, loss in zip(bars, final_losses):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "            f'{loss:.4f}', ha='center', fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Comparison plot saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary Report\n",
    "print(\"=\"*60)\n",
    "print(\"COMPARISON RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nBahdanau Attention:\")\n",
    "print(f\"  Initial Loss: {history_bahdanau['loss'][0]:.4f}\")\n",
    "print(f\"  Final Loss:   {history_bahdanau['loss'][-1]:.4f}\")\n",
    "print(f\"  Improvement:  {history_bahdanau['loss'][0] - history_bahdanau['loss'][-1]:.4f}\")\n",
    "\n",
    "print(f\"\\nLuong Attention:\")\n",
    "print(f\"  Initial Loss: {history_luong['loss'][0]:.4f}\")\n",
    "print(f\"  Final Loss:   {history_luong['loss'][-1]:.4f}\")\n",
    "print(f\"  Improvement:  {history_luong['loss'][0] - history_luong['loss'][-1]:.4f}\")\n",
    "\n",
    "winner = \"Bahdanau\" if history_bahdanau['loss'][-1] \u003c history_luong['loss'][-1] else \"Luong\"\n",
    "diff = abs(history_bahdanau['loss'][-1] - history_luong['loss'][-1])\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Better Performance: {winner} (by {diff:.4f} loss)\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Differences\n",
    "\n",
    "### Bahdanau (Additive) Attention:\n",
    "- Uses feedforward neural network to compute attention scores\n",
    "- More parameters to learn (W1, W2, V)\n",
    "- Can capture more complex alignment patterns\n",
    "- Computationally more expensive\n",
    "\n",
    "### Luong (Multiplicative) Attention:\n",
    "- Uses dot product or simple weight matrix for scores\n",
    "- Fewer parameters (just W for 'general')\n",
    "- Computationally more efficient\n",
    "- Often performs similarly to Bahdanau\n",
    "\n",
    "## Conclusion\n",
    "Both attention mechanisms are effective for sequence-to-sequence tasks. The choice between them often depends on:\n",
    "- Computational budget\n",
    "- Dataset characteristics  \n",
    "- Required model capacity"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
