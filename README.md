# ğŸ§  Seq2Seq Attention Mechanisms: Bahdanau vs Luong

<div align="center">

![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)
![TensorFlow](https://img.shields.io/badge/TensorFlow-2.x-orange.svg)
![License](https://img.shields.io/badge/License-MIT-green.svg)
![Status](https://img.shields.io/badge/Status-Complete-success.svg)

**A comprehensive comparison of Bahdanau (Additive) and Luong (Multiplicative) attention mechanisms in Neural Machine Translation**

[Overview](#overview) â€¢
[Features](#-features) â€¢
[Quick Start](#-quick-start) â€¢
[Results](#-results) â€¢
[Architecture](#-architecture)

</div>

---

## Overview

This project implements and compares two foundational attention mechanisms in sequence-to-sequence models for machine translation:

- **Bahdanau Attention** (Additive) - From ["Neural Machine Translation by Jointly Learning to Align and Translate"](https://arxiv.org/abs/1409.0473)
- **Luong Attention** (Multiplicative) - From ["Effective Approaches to Attention-based Neural Machine Translation"](https://arxiv.org/abs/1508.04025)

Both models are trained on an **English-Portuguese translation task** using identical hyperparameters, enabling a fair side-by-side comparison of their performance characteristics.

## âœ¨ Features

- ğŸ”„ **Complete Seq2Seq Implementation** - Encoder-Decoder architecture with GRU cells
- ğŸ¯ **Dual Attention Mechanisms** - Both Bahdanau and Luong attention fully implemented
- ğŸ“Š **Visual Comparisons** - Training curves and performance visualizations
- ğŸ““ **Jupyter Notebook** - Interactive exploration with step-by-step execution
- ğŸ **Python Script** - Standalone executable for batch processing

## ğŸš€ Quick Start

### Prerequisites

```bash
pip install tensorflow numpy matplotlib
```

### Run with Jupyter Notebook (Recommended)

```bash
jupyter notebook seq2seq_attention_comparison.ipynb
```

Then click **"Run All Cells"** from the Jupyter menu.

### Run as Python Script

```bash
python seq2seq_attention_comparison.py
```

**â±ï¸ Expected Runtime:** 30-60 minutes (depending on hardware)

## ğŸ“‚ Project Structure

```
â”œâ”€â”€ seq2seq_attention_comparison.ipynb   # ğŸ““ Interactive Jupyter notebook
â”œâ”€â”€ seq2seq_attention_comparison.py      # ğŸ Standalone Python script
â”œâ”€â”€ NLP_10.ipynb                         # ğŸ“š Additional NLP experiments
â”œâ”€â”€ README.md                            # ğŸ“– This file
â””â”€â”€ README_COMPARISON.md                 # ğŸ“‹ Detailed usage guide
```

## ğŸ—ï¸ Architecture

### Bahdanau Attention (Additive)

```
score(sâ‚œ, háµ¢) = váµ€ Â· tanh(Wâ‚Â·sâ‚œ + Wâ‚‚Â·háµ¢)
```

- Uses a **feed-forward network** to compute alignment scores
- Concatenates encoder/decoder states before scoring
- More parameters, slightly higher computational cost

### Luong Attention (Multiplicative)

```
score(sâ‚œ, háµ¢) = sâ‚œáµ€ Â· W Â· háµ¢
```

- Uses **dot product** with learned weight matrix
- Directly computes similarity between states
- Fewer parameters, faster computation

### Model Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Encoder   â”‚â”€â”€â”€â”€â–ºâ”‚  Attention  â”‚â”€â”€â”€â”€â–ºâ”‚   Decoder   â”‚
â”‚   (GRU)     â”‚     â”‚   Layer     â”‚     â”‚   (GRU)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â–²                   â”‚                    â”‚
      â”‚                   â–¼                    â–¼
  [Source]          [Context Vector]      [Target]
```

## âš™ï¸ Configuration

Customize training by modifying these parameters:

```python
BATCH_SIZE = 64        # Reduce if running out of memory
EMBEDDING_DIM = 256    # Word embedding dimensions
UNITS = 512            # Hidden layer size
NUM_EXAMPLES = 30000   # Dataset size
EPOCHS = 10            # Training iterations
```

## ğŸ“ˆ Results

After training, you'll receive:

| Metric | Bahdanau | Luong |
|--------|----------|-------|
| Final Loss | ~X.XX | ~X.XX |
| Training Time | Slower | Faster |
| Parameters | More | Fewer |

### Generated Outputs

- `training_comparison.png` - Training loss curves visualization

## ğŸ”§ Troubleshooting

| Issue | Solution |
|-------|----------|
| **Out of Memory** | Reduce `BATCH_SIZE` to 32 or `UNITS` to 256 |
| **Training Too Slow** | Reduce `EPOCHS` to 5 or `NUM_EXAMPLES` to 15000 |
| **No GPU Warning** | Normal - will use CPU (just slower) |

## ğŸ“š Key Learnings

By running this comparison, you'll understand:

1. âœ… How different attention mechanisms perform on the same task
2. âœ… Training dynamics and convergence behavior
3. âœ… Computational trade-offs (speed vs. accuracy)
4. âœ… When to choose one attention type over another

## ğŸ“„ References

- Bahdanau, D., Cho, K., & Bengio, Y. (2014). *Neural Machine Translation by Jointly Learning to Align and Translate*
- Luong, M. T., Pham, H., & Manning, C. D. (2015). *Effective Approaches to Attention-based Neural Machine Translation*

## ğŸ‘¤ Author

**Zewail City of Science and Technology**  
NLP Course - 4th Year Assignment

---

<div align="center">

Made with â¤ï¸ for NLP

</div>
